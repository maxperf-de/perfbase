\documentclass[12pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\author{Joachim Worringen}
\title{perfbase: Design Concept}
\begin{document}

\maketitle

\abstract{perfbase is a system for database-supported management of 
performance measurement results}

\section{Situation}
Each time that a performance-critical part of a software
system is changed (significantly), it is necessary to re-evaluate the
delivered performance. For this purpose, benchmarks are run, and the
output of the benchmarks is transformed into a presentable form like a
chart or a table.

The MPI development at CCRL is a good example for this. The MPI
library consists of many independent parts (subsystems) which are
constantly changed for different reasons:
\begin{itemize}
\item deliver better performance on the same platform
\item change the behaviour of the software, i.e. to adopt to given resource 
  usage restrictions
\item adopt the software to a new hardware platform
\item implement new subsystems that perform new tasks
\end{itemize} 

\section{Problems}
The way that the described performance evaluation is done
now bears a number of problems:
\begin{itemize}
\item Translating the benchmark output into the presentable form is often
  a tedious, manual task as the data needs to be extracted and transfered
  between different software tools.
\item Because of this complexity, the performance measurements are often
  limited in the range of the applicable test parameters, and in the 
  number of samples taken for a certain set of test parameters. This
  leads to results of limited usefulness and statistical correctness.
\item It is complex and error-prone to manage all results in a (big) number of
  files (of a certain type, usally text). It is not easy to discover
  which parts of the parameter set have not yet been measured, of which
  one may need a closer look.
\item Comparing a certain measurement with the same measurement performed
  some time ago is usually difficult. This applies partly because
  the old data is not longer available, can not be found, or didn't
  cover all required parameter sets.
\item It is very difficult for others to access, understand and use the 
  data of one's measurements.
\end{itemize}

\section{Proposed Solution}
To solve the mentioned problems, a higher automatization of
the test procedure will help. This means:
\begin{itemize}
\item Running a script-controlled long/full range of tests, with storing 
  all results/benchmark output in (a single) text file(s).
\item Processing this text file to store all data in a database (which
  has to be set up before?)
\item Performing queries to retrieve specified parts of the data from the 
  database in a certain format (potentially including visualization)
\end{itemize}


\section{Design Concept for Work-flow}
\label{sec:work-flow}
The work-flow for the application of this solution is as described in the following subsections.
\subsection{Experiment Description}

\label{sec:exp-desc}
Create an "experiment description" by definining the test parameters 
("parameter") and the resulting performance value(s) ("results"). 
This experiment will be refered via a unique name ("experiment label")
which is supplied by the user.

\subsection{Database Setup}
\label{sec:db-setup}
Create a new table in the (personal) performance database based on
this description. The experiment description will be stored in the
database.

\subsection{Experiment Run}
\label{sec:exp-run}
Run the experiments and store the results in one or more "result files".

\subsection{Input Description}
\label{sec:inp-desc}
Create a "input description" to extract the performance values from
the result files. 

\subsection{Experiment Data Import}
\label{sec:data-imp}
Import the data into the database. The extracted results will be stored
in the database and form a "run" which has a unique "run label" (either
automatically generated via the date or provided by the user).    
   
A run may contain different sets of paramters! Thus, a run only groups
results to a single import, not to a common set of parameters. The 
grouping to a common set of parameters is done implicitely by the
database (that's what's it for!).

Additionally, the raw result files will be stored for reference.
Potential other uses of having the result files stored in the database:
\begin{itemize}
   \item undo a specific data import (by parsing the result file and deleting
     all matching records).
   \item performing a check to avoid duplicate import from identical result files
\end{itemize}

\subsection{Experiment Update}
\label{sec:exp-up}
Potentially update an experiment with a new experiment description to add
or remove paramters or result fields.
\begin{itemize}
 \item when adding a parameter, a default value needs to be specified which 
       will apply to all existing runs.
 \item when adding a result, this result wil be undefined for all existing runs.
\end{itemize}   

\subsection{Data Retrival and Experiment Analysis}
\label{sec:exp-ana}
Access the database to retrieve information from it. These accesses may
include:
\begin{description}
   \item[Give information]
   \begin{itemize}
     \item general information on the database by listing the available expeirments 
       with number of runs, last modification,...
     \item general information on an experiment in the database by listing the 
       parameters and results, the different runs (with date, run label, 
       number of results, ...)
     \item specific information on an experiment:
      \begin{itemize}
       \item range of a parameter or result
       \item number of result values for a parameter set
      \end{itemize}
   \end{itemize}
   \item["data mining" ("intelligent" data analysis)]
   \begin{itemize}
       \item Is there an equal (sufficient) number of result values for all
         parameter sets?
       \item Are there significant variances on a result value across a single
         parameter set?
       \item Is there a significant variance or a recognizable trend for a result 
         value across multiple runs?
   \end{itemize}
   \item[retrieve data which will be plotted via a "plot description"]
   \begin{itemize}
     \item 2D plot: one result for one parameter varied, all other parameters 
       fixed. 
     \item 2D multiplot: two results (with a separate scale) for one parameter 
       varied, all other parameters fixed. 
     \item 2D parameter-plot: one result for one parameter varied; the plot-
       parameter is another parameter.
     \item 3D plot: one result across two parameters in a 3D-view
     \item general options:
     \begin{itemize}
       \item multiple result values for a set of parameters can 
         be accessed as maximum, minimum, average, quantile, ...
       \item plots can display error bars etc. (using the information from multiple
         result values per parameter set)
       \item \emph{1-D plot series}: a number of distinct plots with identical parameter sets
         across the variation of one parameter
       \item \emph{2-D plot series}: a number of distinct plots with identical parameter sets
         across the variation of two parameters
       \item store a plot in the database, using a "plot label" and a "plot comment"
     \end{itemize}
   \end{itemize}
   \end{description} 

\subsection{Database Management}
\label{sec:db-mngmt}
Manage the database:
\begin{itemize}
\item general access rights etc. for the database are managed "externally" by
     the default means for this.
\item delete an experiment
\item delete a run from an experiment
\item export (parts of) an experiment as ASCII-dump
\item store, retrieve or delete "meta information" (input and plot descriptions)
\end{itemize}


\section{Implementation}

\subsection{Language}
'perl' was chosen as the implementation language. Reasons:
\begin{itemize}
 \item backend-independent database usage via DBI (well,nearly...)
 \item good string/text processing functions
 \item portable \& free
 \item no compilation, but (hopefully) fast enough
 \item I want to learn it
\end{itemize}

\subsection{Workflow steps (see section \ref{sec:work-flow})}
\subsubsection{Step \ref{sec:exp-desc} (Experiment Description)}
   The experiment description is contained in a .exp file which contains 
   entries according to the following syntax:

   General:
   \begin{itemize}
   \item lines starting with \# are comments (ignored)
   \item empty lines are ignored
   \item all other lines between to lines which start with a keyword are comments
     for the last keyword which will be stored in the database
   \end{itemize}

   Keywords:
   \begin{itemize}
   \item "*database" <host:port> <database name>
   \item "*experiment" <label>   
     (label needs to be unique within database)
   \item "*read" <list,of,users,with,read,access> 
     (optional, default specified by database)
   \item "*write" <list,of,users,with,write,access> 
     (optional, default specified by database)
   \item "*parameter" <name> <data type> <unit> <default> 
     (at least one parameter is required)
   \item "*result" <name> <data type> <unit>
     (at least one result value is required)
   \end{itemize}

   Supported data types are \texttt{int}, \texttt{float}, \texttt{string} and \texttt{date}.
   Units only apply to data types int and float. Supported units are Byte, s,
   Byte/s, FlOP, FLOP/s, OP, OP/s. Only for supported units does the multiplication
   and division result in the according units. Supported multipliers are \texttt{K} ($10^3$), 
   \texttt{M} ($10^6$), \texttt{G} ($10^9$), \texttt{T} ($10^12$), \texttt{P} ($10^15$), \texttt{m} ($10^-3$), \texttt{u} ($10^-6$),
   \texttt{n} ($10^-9$), \texttt{p} ($10^-12$) and \texttt{f} ($10^-15$). Additionally, supported base-2 
   multipliers are \texttt{Ki} ($2^10$), \texttt{Mi} ($2^20$), \texttt{Gi} ($2^30$), \texttt{Ti} ($2^40$) and
   \texttt{Pi} ($2^50$). 

   Recommended naming scheme for the paramters and results is $A_b$ with
   A characterising the type of variable and b specifying what it
   exactly does represent. Usual types of variables are: 
   \begin{itemize}
   \item 'B': bandwidth, amount of data transported per time unit
   \item 'L': latency, a time span used to transport data
   \item 'T': also a time span, but describing any kind of task
   \item 'S': size of a single object, like a file or a message
   \item 'N': number of objects in a group, like number of nodes
   \end{itemize}
   
\subsubsection{Step \ref{sec:db-setup} (Database Setup)}
   Once the description is complete, the experiment can be created
   via \texttt{pb\_create}. To update an existing experiment, the same type
   of description is used as input for "pb\_update". The database
   itself needs to be created using the usual external tools provided
   with the database implementation used.

\subsubsection{Step \ref{sec:exp-run} (Experiment Run)}
   Run the desired tests. If possible, the ouput of the test should be
   defined in a way which eases (or allows!) the parsing of the output
   for the results and parameter values.

\subsubsection{Step \ref{sec:inp-desc} (Input Description)}
   This is complex. There are different ways how this definition can be 
   performed, depending on how the result file is formatted.
   
   First, the result files. Each result file may contain one or more
   "input sets". Each input set uses exactly one parameter set. Multiple
   input set in a results file are separated with an "input separator"
   which is a string. Alternatively, 

   \begin{itemize}
   \item Input set separation:
     Keyword: SEP
     Descriptors: string        a fixed partial string 
                  param         a parameter to indicate a new input set.
                  alias         an alias for this parameter (multiple)
                  ignore        

     Examples:
\begin{verbatim}
     SEP string="*****"
\end{verbatim}
     (any row containing ***** marks a new input set)     
\begin{verbatim}
     SEP param=S\_cache alias=cachesize
\end{verbatim}
     (any row which defines a new value for S\_cache (see "Named location" below)
      marks the start of a new input set, using this value for the  parameter)

   Single value per input set:
   - Named location: The value is marked in the result file with a 
     preceeding string which matches either the value name itself or
     a assigned alias of the value name.

     Keyword: NAMED
     Descriptors: len     length of value to use 
                          (for strings only, 0 means "up to EOL")
                  alias   an alias for this parameter (multiple)
                  ws      a string of space-separated chars which will
                          be considered as whitespaces, too

     Example:
\begin{verbatim}
     NAMED N\_proc alias=NP ignore="= :"
     (in a row which contains NP=5, or NP: 5, it will set N\_proc to 5)

   - Explicit location: A value is the n-th value in a row of the input set 
     which is specified either by its number in the input
     file or by a string which is contained in the row.
     Optionally, only the same type of values are counted when looking for
     the n-th value (say, integer or float numbers).
     
     Keyword: EXPL
     Descriptors: row   row to use (by number or keyword)
                  pos   position in this row (by number or keyword)
                  rep   if pos uses keyword, look for 'rep' occurances of it
                  typed only count value appearances of the same type
                  len   length of value to use (for strings only)
                        0 means "up to EOL"

     Like:
\begin{verbatim}
     ROW N\_proc row=13 pos=5
\end{verbatim}
     (will use the 5th entry in row 13)
\begin{verbatim}
     ROW N\_proc row="number of processes"  pos=":" rep=5
\end{verbatim}
     (will use the first row which contains "number of processes"; within this
      row, the value behind the 5th occurence of ':' will be used)

   - Fixed: a value may be given as a fixed value (or passed as argument 
     to pb\_import and is then fixed for all input sets.

     Keyword: FIXED
     Descriptors: none

     Example:
\begin{verbatim}
     FIXED N\_proc 16
\end{verbatim}
     (N\_proc will have the value 16 for all input sets)

   Multipe values per input set (only applies for result values):
   - Tabular location: The values are found in the n-th column of a table 
     which starts either at a fixed line in the result file, or below a 
     fixed string. 

     Parsing of the table stops once a row with a different number of entries
     is found, or if the selected entry does not match the type of the value.

     Keyword: TABLE
     Descriptors: row   row *after* which the table starts (by number or keyword)
                  pos   position in this row (by number or keyword)
                  rep   if pos uses keyword, look for 'rep' occurances of it
                  typed only count value appearances of the same type
                  len   length of value to use (for strings only)
                        0 means "up to EOL"
   
\subsubsection{Step \ref{sec:data-import} (Data Import)}
   The import itself is performed by \texttt{pb\_import}. It will use the following
   syntax:

\begin{verbatim}
   pb\_import -i <input\_desc> [-d <exp\_desc> -h <db\_host> -p <db\_port> -n <db\_name> 
     -u <db\_user> -e <experiment>] [-t] [-v] <input files>}
\end{verbatim}

   \texttt{input\_desc} is the input description file which is mandatory. To specify the
   experiment (and the database), the user can either supply the required information 
   via the experiment description file (-d) or via separate options (-h, -p, -n, -u, -e).
   Finally, to test the import, -t can be specified which prints the values to be 
   imported on stdout. An arbitrary number of input file names can follow the options.
   The "verbose" option (-v) provides some information on the imported data.

\subsubsection{Step \ref{sec:exp-up} (Experiment Update}
   Use \texttt{pb\_update} which adds new columns (and sets default values if required), or
   deletes columns. 

\subsubsection{Step \ref{sec:exp-ana} (Experiment Analysis}
\paragraph{Information Retrieval}
   \texttt{pb\_info}

\paragraph{Data Mining}
   \texttt{pb\_mine}

\paragraph{Data Plotting}
\begin{verbatim}
    pb\_plot -p <plot\_desc> -d <exp\_desc> [-o <ouput\_file>]
\end{verbatim}

    The plotting is controlled by a \emph{plot description file} which has
    the following entries:

    TYPE 2D|2D-multi|2D-param|3D

    Keyword: RESULT 
    Descriptors:    min
                    max
                    

\subsubsection{Step \ref{sec:db-mngmt} (Database Management)}
\paragraph{Deleting data} To delete data from the database, use \texttt{pb\_delete}. Syntax:
\begin{verbatim}
     pb\_delete -e <experiment> [-r <run>] [-f]
\end{verbatim}

\paragraph{Export data} To export data from the database, use \texttt{pb\_export}. Syntax:
\begin{verbatim}
     pb\_export -e <experiment> [-r <run>] [-x <exp_desc>] 
\end{verbatim}         
     An "export description" is a 



\end{document}
